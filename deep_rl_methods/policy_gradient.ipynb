{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from tensorflow.contrib.layers import fully_connected as fc\n",
    "from tensorflow.contrib.layers import xavier_initializer as xavier\n",
    "\n",
    "import gym\n",
    "import gym.spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "# set some random seed for reproducibility\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_module(input_layer, hiddens, activation_fn=tf.nn.relu):\n",
    "    \"\"\" fully connected module\n",
    "        hiddens: list of form [n_1, n_2, ..., n_k] where\n",
    "        n_i is the number of neurons on i_th hidden layer\n",
    "    \"\"\"\n",
    "    out = input_layer\n",
    "    for num_outputs in hiddens:\n",
    "        out = fc(\n",
    "            out,\n",
    "            num_outputs=num_outputs,\n",
    "            activation_fn=activation_fn,\n",
    "            weights_initializer=xavier())\n",
    "    return out\n",
    "\n",
    "\n",
    "class Policy:\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            state_shape,\n",
    "            num_actions,\n",
    "            hiddens=[16, 16],\n",
    "            learning_rate=1e-3,\n",
    "            scope=\"policy\"):\n",
    "\n",
    "        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "\n",
    "            # Define placeholders for states, actions and targets\n",
    "            self.states_ph = tf.placeholder(\n",
    "                dtype=tf.float32, shape=(None,)+state_shape)\n",
    "            self.actions_ph = tf.placeholder(\n",
    "                dtype=tf.int32, shape=(None, 1))\n",
    "            self.targets_ph = tf.placeholder(\n",
    "                dtype=tf.float32, shape=(None, 1))\n",
    "\n",
    "            # Construct network graph\n",
    "            out = fc_module(self.states_ph, hiddens)\n",
    "            self.logits = fc_module(out, [num_actions], activation_fn=None)\n",
    "            self.probs = tf.squeeze(tf.nn.softmax(self.logits))\n",
    "\n",
    "            # Construct loss as softmax cross entropy with logits\n",
    "            actions_onehot = tf.one_hot(self.actions_ph, depth=num_actions)\n",
    "            neg_likelihoods = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                labels=actions_onehot, logits=self.logits)\n",
    "            self.loss = tf.reduce_sum(\n",
    "                tf.multiply(neg_likelihoods, self.targets_ph))\n",
    "\n",
    "            # Set optimizer and training operation\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def __call__(self, sess, states):\n",
    "        \"\"\" estimate vector of action probabilities (policy)\n",
    "            for a given batch of states\n",
    "        \"\"\"\n",
    "        return sess.run(self.probs, {self.states_ph: states})\n",
    "    \n",
    "    def train(self, sess, states, actions, targets):\n",
    "        feed_dict = {\n",
    "            self.states_ph: states,\n",
    "            self.actions_ph: actions,\n",
    "            self.targets_ph: targets}\n",
    "        loss, _ = sess.run([self.loss, self.train_op], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "p = Policy(state_shape, num_actions, hiddens=[128, 128], learning_rate=1e-3)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = lambda state: p(sess, state[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(env, policy, num_episodes=10, gamma=0.97):\n",
    "    \"\"\" generate batch of transitions for training\n",
    "    \"\"\"\n",
    "    \n",
    "    states = []\n",
    "    actions = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        rewards = []\n",
    "        while not done:\n",
    "            probs = policy(s)\n",
    "            a = np.random.choice(num_actions, p=policy(s))\n",
    "            s_, r, done, _ = env.step(a)\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "            s = s_\n",
    "        total_reward = sum(rewards)\n",
    "        for i in range(1, len(rewards)):\n",
    "            rewards[i] = rewards[i] + gamma * rewards[i-1]\n",
    "        targets += rewards[::-1]\n",
    "    \n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)[:, None]\n",
    "    targets = np.array(targets)[:, None]\n",
    "    \n",
    "    return states, actions, targets, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "avg_reward = 0\n",
    "for ep in range(1000):\n",
    "    states, actions, targets, total_reward = generate_batch(env, policy, batch_size, gamma=0.99)\n",
    "    avg_reward += total_reward\n",
    "    loss = p.train(sess, states, actions, targets)\n",
    "    if ep % 50 == 0:\n",
    "        print (\"Number of episodes:\", ep)\n",
    "        print (\"Average reward over last 50 episodes:\", avg_reward/50)\n",
    "        print (\"--------------------------------------------\")\n",
    "        avg_reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(env, policy, num_episodes):\n",
    "    total_reward = 0\n",
    "    for ep in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        for i in range(500):\n",
    "            a = np.random.choice(num_actions, p=policy(s))\n",
    "            s_, r, done, _ = env.step(a)\n",
    "            total_reward += r\n",
    "            s = s_\n",
    "            if done:\n",
    "                break\n",
    "    avg_reward = total_reward / num_episodes\n",
    "    print (\"Average reward over {} episodes is {}\".format(num_episodes, avg_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy(env, policy, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "for i in range(500):\n",
    "    a = np.random.choice(num_actions, p=policy(s))\n",
    "    s_, r, done, _ = env.step(a)\n",
    "    s = s_\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "print (i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opensim",
   "language": "python",
   "name": "opensim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
