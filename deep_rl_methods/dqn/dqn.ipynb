{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexgrinch/miniconda3/envs/opensim-rl/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/alexgrinch/miniconda3/envs/opensim-rl/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from experience_replay import ReplayBuffer\n",
    "from deep_q_network import DeepQNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            environment,\n",
    "            num_actions,\n",
    "            agent_network,\n",
    "            target_network):\n",
    "        self.env = environment\n",
    "        self.num_actions = num_actions\n",
    "        self.agent_net = agent_network\n",
    "        self.target_net = target_network\n",
    "\n",
    "    def set_parameters(\n",
    "            self,\n",
    "            replay_buffer_size=100000,\n",
    "            replay_initial_size=10000,\n",
    "            start_epsilon=1.0,\n",
    "            final_epsilon=0.01,\n",
    "            annealing_steps=100000,\n",
    "            discount_factor=0.99,\n",
    "            max_episode_length=1000):\n",
    "\n",
    "        # create replay buffer\n",
    "        self.rep_buffer = ReplayBuffer(replay_buffer_size)\n",
    "        # fill replay buffer with experience generated by random policy\n",
    "        s = self.env.reset()\n",
    "        while self.rep_buffer.stored_in_buffer < replay_initial_size:\n",
    "            a = np.random.randint(self.num_actions)\n",
    "            s_, r, done = self.env.step(a)[:3]\n",
    "            self.rep_buffer.push_transition((s, a, r, s_, done))\n",
    "            if done:\n",
    "                s = self.env.reset()\n",
    "            else:\n",
    "                s = s_\n",
    "        # define epsilon decay schedule\n",
    "        self.eps = start_epsilon\n",
    "        self.final_eps = final_epsilon\n",
    "        self.delta_eps = (start_epsilon - final_epsilon) / annealing_steps\n",
    "        # set discount factor and maximum length of the episode\n",
    "        self.gamma = discount_factor\n",
    "        self.max_ep_length = max_episode_length\n",
    "        \n",
    "    def update_target_net(self):\n",
    "        update_ops = []\n",
    "        for v_agnt, v_trgt in zip(self.agent_net.vars, self.target_net.vars):\n",
    "            update_ops.append(v_trgt.assign(v_agnt))\n",
    "        self.sess.run(update_ops)\n",
    "\n",
    "    def train(\n",
    "            self,\n",
    "            batch_size,\n",
    "            agent_update_frequency=4,\n",
    "            target_update_frequency=5000,\n",
    "            max_num_episodes=50000):\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        episode_num = 0\n",
    "        self.rewards_history = []\n",
    "        global_time_step = 0\n",
    "        while episode_num < max_num_episodes:\n",
    "            \n",
    "            s = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            for time_step in range(self.max_ep_length):\n",
    "                # pick action epsilon-greedily\n",
    "                if np.random.rand() < self.eps:\n",
    "                    a = np.random.randint(self.num_actions)\n",
    "                else:\n",
    "                    a = self.agent_net.get_greedy_action(self.sess, [s])\n",
    "                # make a step in the environment\n",
    "                s_, r, done = env.step(a)[:3]\n",
    "                episode_reward += r\n",
    "                # save transition into replay buffer\n",
    "                self.rep_buffer.push_transition((s, a, r, s_, done))\n",
    "                \n",
    "                # update agent weights\n",
    "                if global_time_step % agent_update_frequency == 0:\n",
    "                    batch = self.rep_buffer.get_batch(batch_size)\n",
    "                    next_actions = self.target_net.get_greedy_action(\n",
    "                        self.sess, batch.s_)\n",
    "                    next_q_values = self.target_net.get_q_values(\n",
    "                        self.sess, batch.s_, next_actions)\n",
    "                    targets = batch.r + self.gamma * next_q_values * (1-batch.done)\n",
    "                    loss = self.agent_net.train(\n",
    "                        self.sess, batch.s, batch.a, targets)\n",
    "                    \n",
    "                # update target weights\n",
    "                if global_time_step % target_update_frequency == 0:\n",
    "                    self.update_target_net()\n",
    "                    \n",
    "                # decay epsilon\n",
    "                self.eps = max(self.final_eps, self.eps - self.delta_eps)\n",
    "                \n",
    "                global_time_step += 1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                s = s_\n",
    "\n",
    "            episode_num += 1\n",
    "            self.rewards_history.append(episode_reward)\n",
    "            if episode_num % 500 == 0:\n",
    "                avg_reward = np.mean(self.rewards_history[-500:])\n",
    "                print (\"Average reward over 500 episodes: {}\".format(avg_reward))\n",
    "                print (\"Epsilon: {}\".format(self.eps))\n",
    "                print (\"-------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snake import Snake\n",
    "env = Snake(grid_size=(6, 6))\n",
    "num_actions = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "agent_net = DeepQNetwork(\n",
    "    num_actions,\n",
    "    state_shape=[6, 6, 1],\n",
    "    convs=[[16, 2, 1], [32, 1, 1]],\n",
    "    scope=\"agent\")\n",
    "target_net = DeepQNetwork(\n",
    "    num_actions,\n",
    "    state_shape=[6, 6, 1],\n",
    "    convs=[[16, 2, 1], [32, 1, 1]],\n",
    "    scope=\"target\")\n",
    "\n",
    "agent = RLAgent(env, num_actions, agent_net, target_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.set_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 500 episodes: -0.864\n",
      "Epsilon: 0.9711711999999793\n",
      "-------------------------\n",
      "Average reward over 500 episodes: -0.874\n",
      "Epsilon: 0.9409464999999575\n",
      "-------------------------\n",
      "Average reward over 500 episodes: -0.83\n",
      "Epsilon: 0.9074151999999334\n",
      "-------------------------\n",
      "Average reward over 500 episodes: -0.83\n",
      "Epsilon: 0.873586899999909\n",
      "-------------------------\n",
      "Average reward over 500 episodes: -0.794\n",
      "Epsilon: 0.837392499999883\n",
      "-------------------------\n",
      "Average reward over 500 episodes: -0.8\n",
      "Epsilon: 0.7978617999998545\n",
      "-------------------------\n",
      "Average reward over 500 episodes: -0.718\n",
      "Epsilon: 0.7502724999998203\n",
      "-------------------------\n",
      "Average reward over 500 episodes: -0.728\n",
      "Epsilon: 0.7028217999997861\n",
      "-------------------------\n",
      "Average reward over 500 episodes: -0.7\n",
      "Epsilon: 0.6425802999997428\n",
      "-------------------------\n",
      "Average reward over 500 episodes: -0.702\n",
      "Epsilon: 0.5821011999996992\n",
      "-------------------------\n",
      "Average reward over 500 episodes: -0.554\n",
      "Epsilon: 0.5147415999996507\n",
      "-------------------------\n",
      "Average reward over 500 episodes: -0.514\n",
      "Epsilon: 0.4353237999995936\n",
      "-------------------------\n",
      "Average reward over 500 episodes: -0.4\n",
      "Epsilon: 0.3529458999995343\n",
      "-------------------------\n",
      "Average reward over 500 episodes: -0.122\n",
      "Epsilon: 0.24156099999945413\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 0.216\n",
      "Epsilon: 0.06081669999941284\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 1.05\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 1.52\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 2.102\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 2.52\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 3.736\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 4.856\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 6.296\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 7.632\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 9.036\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 9.746\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 10.294\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 10.9\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 11.722\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 11.89\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 12.202\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 12.626\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 12.618\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 13.56\n",
      "Epsilon: 0.01\n",
      "-------------------------\n",
      "Average reward over 500 episodes: 13.666\n",
      "Epsilon: 0.01\n",
      "-------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-489b393dc6ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-80813c74821a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size, agent_update_frequency, target_update_frequency, max_num_episodes)\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_greedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;31m# make a step in the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0ms_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;31m# save transition into replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/projects/rl_course_acaml2018/deep_rl_methods/dqn/snake.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \"\"\"\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/projects/rl_course_acaml2018/deep_rl_methods/dqn/snake.py\u001b[0m in \u001b[0;36mupdate_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mx_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0my_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opensim",
   "language": "python",
   "name": "opensim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
